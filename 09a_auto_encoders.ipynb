{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTY-Msjal7zU"
   },
   "source": [
    "# Generative models - auto-encoders\n",
    "\n",
    "### Author: Philippe Esling (esling@ircam.fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z3M5ezRyrjA3"
   },
   "source": [
    "In this course we will cover\n",
    "1. A brief introduction to [generative models](#generative)\n",
    "2. A formal presentation of [Auto-Encoders](#ae) (AEs)\n",
    "3. An explanation of how to [implement AEs](#implement)\n",
    "4. An [application](#application) of AEs for modeling images\n",
    "4. An practical exemple of [convolutional denoising AEs](#denoising) for image data **(exercise)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"generative\"></a>\n",
    "\n",
    "## Generative models\n",
    "\n",
    "### Supervised refresher\n",
    "\n",
    "Until now, we have mostly discussed models that are developed for _supervised_ and _discriminative_ tasks. To formalize this problem, we have a set of data $\\{\\mathbf{x}_{i}, \\mathbf{y}_{i}\\}_{i\\in[1,n]}$, where the $\\mathbf{x}_{i}$ and $\\mathbf{y}_{i}$ are linked. Therefore, we want to approximate this relation through\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\hat{y}} = \\mathcal{F}_\\mathbf{\\theta}(\\mathbf{x})  \n",
    "\\end{equation}\n",
    "where we train the parameters $\\mathbf{\\theta}$ so that $\\mathbf{\\hat{y}}\\approx\\mathbf{y}$. The existence of a label $\\mathbf{y}$ (\"correct answer\") defines a _supervised_ problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going unsupervised\n",
    "\n",
    "In some cases, we might only have a set of data $\\{\\mathbf{x}_{i}\\}_{i\\in[1,n]}$, and still be interested in learning some underlying properties or structure of this set. In that case, the problem is _unsupervised_ as we have to learn without a given answer. \n",
    "\n",
    "Here, we can turn to _generative_ models [[1](#reference1)], which allows to create new data instances based on the observation of existing examples. Although these models are more naturally defined in a _probabilistic way_, we will assume that we have some simple _code_ $\\mathbf{z}$, which allows to control the properties of the generation, and need to learn\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\hat{x}} = \\mathcal{F}_\\mathbf{\\theta}(\\mathbf{z})  \n",
    "\\end{equation}\n",
    "where we still need to learn $\\mathbf{\\theta}$, so that $\\mathbf{\\hat{x}}$ have similar properties to that of the examples in $\\{\\mathbf{x}_{i}\\}_{i\\in[1,n]}$.\n",
    "\n",
    "Now the problem to solve is how we can learn directly from a set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kg83oWxMm92f"
   },
   "source": [
    "<a id=\"ae\"></a>\n",
    "\n",
    "## Auto-encoders\n",
    "\n",
    "One way to understand a set of data is to try to _compress_, or _simplify_ the corresponding dataset. So the idea is to learn simultaneously how to _encode_ our unlabeled input $\\{\\mathbf{x}_{i}\\}_{i\\in[1,n]}$ and to _decode_ the corresponding representation. This idea give rise to the notion of **auto-encoder**. \n",
    "\n",
    "### Architecture \n",
    "\n",
    "The auto-encoder is an unsupervised architecture originally proposed to perform _dimensionality reduction_ [[3](#reference3)]. As its name indicates, we will try to train this model to learn an efficient _encoding_ $\\mathbf{z}$ of unlabeled input data $\\mathbf{x}$. The only way to learn efficient parameters is to also learn a _decoding_ function to _reconstruct_ $\\mathbf{x}$ from $\\mathbf{z}$.\n",
    "\n",
    "<img src=\"images/auto_encoder.png\"/>\n",
    "\n",
    "As shown here, a first model $\\mathcal{E}_\\phi$ _encodes_ the input into a _latent code_ $\\mathbf{z}$ in order to provide a low-dimensional representation of the data. A second model $\\mathcal{D}_\\theta$ designated as the _decoder_ aims to generate outputs from $\\mathbf{z}$ that are as close to the original inputs as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formal definition\n",
    "\n",
    "The latent code $\\mathbf{z}$ can be seen as a compressed abstract representation, and may be used as an intermediate space for analysis or generation. This helps to govern the distribution of the data through a simpler and higher-level representation, while enhancing the _expressiveness_ of the generative model.\n",
    "The behaviour of an auto-encoder can be formalized as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z} &= \\mathcal{E}_\\phi(\\mathbf{x}) \\\\\n",
    "\\mathbf{\\hat{x}} &= \\mathcal{D}_\\theta(\\mathbf{z})  \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "with the _encoder_ $\\mathcal{E}_\\phi$ and _decoder_ $\\mathcal{D}_\\theta$ functions parameterized respectively by $\\phi$ and $\\theta$. As we can see this defines the reconstruction relationship\n",
    "$$\n",
    "    \\mathbf{\\hat{x}} = \\mathcal{D}_\\theta(\\mathcal{E}_\\phi(\\mathbf{x}))  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The training of an auto-encoder consists in finding the optimal functions of encoding $\\mathcal{E}^*$ and decoding $\\mathcal{D}^*$ by evaluating the \\textit{reconstruction error} $\\mathcal{L}$ between $\\mathbf{x}$ and $\\mathbf{\\hat{x}}$, such that\n",
    "\\begin{equation}\n",
    "    \\mathcal{E}^*, \\mathcal{D}^* = arg\\,min_{ \\phi, \\theta}{\\mathcal{L}}(\\mathbf{x}, \\mathcal{D}_\\theta(\\mathcal{E}_\\phi(\\mathbf{x})))\n",
    "\\end{equation}\n",
    "\n",
    "As the latent space usually has a smaller dimensionality than the input, it acts as an incentive for the network to find the main attributes of variations in the dataset (and also explains its use for _dimensionality reduction_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variants and discussion\n",
    "\n",
    "There are several variants of auto-encoders, such as denoising auto-encoders or variational auto-encoders. Each address some downside of the basic AE model. For instance, the deterministic nature of the basic auto-encoder implies a point-wise mapping of the latent space, meaning that not all the latent positions can be leveraged to produce relevant reconstructions. Because of this reason, there is no way to ensure that the latent space could allow a robust generalization and that any random $\\mathbf{z}$ would generate a meaningful output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cAxMjUCBUa43"
   },
   "source": [
    "<a id=\"implement\"> </a>\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Here, we discuss how we can implement and train a simple auto-encoder network in Pytorch. As discussed earlier, an AE is composed of two parts, an **encoder** and a **decoder**. The goal of the encoder is to \"compress\" the dataset, representing its principal features with a very small code, while the goal of the decoder is to learn how to reproduce the initial input from this code. Hence, we will first need to use some basic imports and definition to setup our problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1_Y75QXJS6h"
   },
   "source": [
    "### Import Pytorch and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YfIk2es3hJEd"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from helper_plot import hdr_plot_style\n",
    "hdr_plot_style();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYn4MdZnKCey"
   },
   "source": [
    "### Load the dataset\n",
    "\n",
    "To start with a pragmatic and simple to understand example, we  will try to train the basic AE using the Fashion MNIST dataset. This dataset contains images of size 28x28 pixels, with different pieces of clothing. The following code allows to load (and eventually download) the dataset, by using the `torchvision.datasets` module. We also plot some randomly selected test examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YZm503-I_tji"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "9.6%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\FashionMNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "dataset_dir = './data'\n",
    "# Going to use 80%/20% split for train/valid\n",
    "valid_ratio = 0.2\n",
    "# Load the dataset for the training/validation sets\n",
    "train_valid_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "# Split it into training and validation sets\n",
    "nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset))\n",
    "nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "# Load the test set\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, transform=torchvision.transforms.ToTensor(),train=False)\n",
    "# Prepare \n",
    "num_threads = 4     # Loading the dataset is using 4 CPU threads\n",
    "batch_size  = 128   # Using minibatches of 128 samples\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_threads)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_threads)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,num_workers=num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to check the properties of our different sets and also plot some random examples, in order to better understand what type of data we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train set contains 48000 images, in 375 batches\n",
      "The validation set contains 12000 images, in 94 batches\n",
      "The test set contains 10000 images, in 79 batches\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The train set contains {} images, in {} batches\".format(len(train_loader.dataset), len(train_loader)))\n",
    "print(\"The validation set contains {} images, in {} batches\".format(len(valid_loader.dataset), len(valid_loader)))\n",
    "print(\"The test set contains {} images, in {} batches\".format(len(test_loader.dataset), len(test_loader)))\n",
    "nsamples = 10\n",
    "classes_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal','Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "imgs_test, labels = next(iter(train_loader))\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "for i in range(nsamples):\n",
    "    ax = plt.subplot(1,nsamples, i+1)\n",
    "    plt.imshow(imgs_test[i, 0, :, :], vmin=0, vmax=1.0, cmap=matplotlib.cm.gray)\n",
    "    ax.set_title(\"{}\".format(classes_names[labels[i]]), fontsize=15)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEdCXSwCoKok"
   },
   "source": [
    "## Basic auto-encoder\n",
    "\n",
    "We recall here that in order to define an autoencoder, we will need an `encoder`, which compresses the images into a small latent vector, and a `decoder`, that reconstructs the original image from this code. Here, we will start very basic and define the encoder and decoder as simple `Dense` layers. To define the model simply, we will use the [Keras API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) defined in the `tensorflow.keras` module (Note that we pre-loaded the `layers` submodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(AE, self).__init__()\n",
    "        self.latent_dim = encoding_dim\n",
    "        self.hidden_dim = 256\n",
    "        self.encoder = nn.Sequential(\n",
    "          nn.Linear(28 * 28, self.hidden_dim), nn.ReLU(),\n",
    "          nn.Linear(self.hidden_dim, self.hidden_dim), nn.ReLU(),\n",
    "          nn.Linear(self.hidden_dim, self.latent_dim), nn.Sigmoid()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "          nn.Linear(self.latent_dim, self.hidden_dim), nn.ReLU(),\n",
    "          nn.Linear(self.hidden_dim, self.hidden_dim), nn.ReLU(),\n",
    "          nn.Linear(self.hidden_dim, 28 * 28), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x.reshape(-1, 28 * 28))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the model depends on a given `encoding_dim` variable, which defines the size of the latent code. Therefore, we can instantiate our model arbitrarliy with `64` dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0MUxidpyChjX"
   },
   "outputs": [],
   "source": [
    "latent_dim = 64   \n",
    "model = AE(latent_dim) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only remaining part that we did not discuss yet is what type of _loss_ (defined as $\\mathcal{L}$) we can use to train our model. First, we will simply rely on the _Mean Squared Error_ (MSE) loss, which is defined as\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}_{MSE}(\\hat{\\mathbf{x}}, \\mathbf{x}) = \\mid \\hat{\\mathbf{x}}, \\mathbf{x} \\mid^{2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9I1JlqEIDCI4"
   },
   "outputs": [],
   "source": [
    "# Loss function that we will use\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oJSeMTroABs"
   },
   "source": [
    "<a id=\"application\"> </a>\n",
    "\n",
    "### Training the model\n",
    "\n",
    "Train the model using `x_train` as both the input and the target. The `encoder` will learn to compress the dataset from 784 dimensions to the latent space, and the `decoder` will learn to reconstruct the original images.\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "h1RI9OfHDBsK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5487.1987, grad_fn=<MseLossBackward0>)\n",
      "tensor(4607.8984, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, x)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Before the backward pass, zero all of the network gradients\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass: compute gradient of the loss with respect to parameters\u001b[39;00m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:237\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m foreach:\n\u001b[0;32m    236\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\profiler.py:446\u001b[0m, in \u001b[0;36mrecord_function.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_enter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_ops.py:143\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Learning rate\n",
    "learning_rate = 1e-4\n",
    "# Optimizer to fit the weights of the network\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "for t in range(20):\n",
    "    full_loss = torch.Tensor([0])\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    for i, (x, _) in enumerate(train_loader):\n",
    "        y_pred = model(x)\n",
    "        # Compute the loss.\n",
    "        loss = loss_fn(y_pred, x)\n",
    "        # Before the backward pass, zero all of the network gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass: compute gradient of the loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Calling the step function to update the parameters\n",
    "        optimizer.step()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAM1QBhtoC-n"
   },
   "source": [
    "Now that the model is trained, we can test it by encoding and decoding images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pbr5WCj7FQUi"
   },
   "outputs": [],
   "source": [
    "encoded_imgs = model.encoder(imgs_test.reshape(-1,28*28))\n",
    "decoded_imgs = model.decoder(encoded_imgs).reshape(-1,28,28).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the images, we can see that the model is able to perform an adequate (yet somewhat blurry) reconstruction of the input images. The interesting point is that this reconstruction comes from a code of only `64` dimensions, whereas the original images have `784` dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4LlDOS6FUA1"
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(imgs_test[i, 0])\n",
    "    plt.title(\"original\"); plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i])\n",
    "    plt.title(\"reconstructed\"); plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements\n",
    "\n",
    "Even though this very basic example seems to work, several improvements can be made over the original model. First, we can see that the overall framework does not depend on the exact nature of the `encoder` and `decoder`. \n",
    "\n",
    "***\n",
    "1. We rewrite the original class to accept any type of architecture for these (see code below). \n",
    "2. Fill the missing code to have a new AE model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoding_dim):\n",
    "        super(AE, self).__init__()\n",
    "        self.latent_dim = encoding_dim\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "\n",
    "encoder = ...\n",
    "decoder = ...\n",
    "model = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we only used the train dataset, whereas the use of a validation and test sets allows to ensure that we do not overfit our model.\n",
    "\n",
    "***\n",
    "1. Write a separate train function, that can be called on different sets\n",
    "2. Re-write the training loop to test overfitting\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4gv6G8PoRQE"
   },
   "source": [
    "## Exercise: Denoising AE\n",
    "\n",
    "Imagine (for the sake of argument), that we choose an encoding dimension which is of same dimensionality as the input one. Then, one huge problem is that nothing prevents the AE from simply learning the _identity_ function (try to imagine why). An autoencoder can also be trained to remove noise from images. This type of _regularization_ prevents the model from learning this degenerate situation.\n",
    "\n",
    "In this exercise, you will need to create your own denoising AE, by relying on a noisy version of the Fashion MNIST dataset (adding random Gaussian noise to each image). You will then train an autoencoder using the noisy image as input, and the original image as the target.\n",
    "\n",
    "Let's reimport the dataset to omit the modifications made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJZ-TcaqDBr5"
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPZl_6P65_8R"
   },
   "source": [
    "Here, we create two new train and test sets by adding random noise to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "axSMyxC354fc"
   },
   "outputs": [],
   "source": [
    "# Load the datasets and use our Gaussian noise transform\n",
    "train_valid_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, train=True, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), GaussianNoise(0., 0.1)]), download=True)\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root=dataset_dir, transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(), GaussianNoise(0., 0.1)]),train=False)\n",
    "# Create loaders\n",
    "train_loader_noisy = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_threads)\n",
    "valid_loader_noisy = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_threads)\n",
    "test_loader_noisy = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False,num_workers=num_threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRxHe4XXltNd"
   },
   "source": [
    "Plot the noisy images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thKUmbVVCQpt"
   },
   "outputs": [],
   "source": [
    "x_test, labels = next(iter(test_loader))\n",
    "x_test_noisy, labels = next(iter(test_loader_noisy))\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.title(\"original\")\n",
    "    plt.imshow(x_test[i].squeeze())\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.gray()\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.title(\"original + noise\")\n",
    "    plt.imshow(x_test_noisy[i].squeeze())\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.gray()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy9SY8jGl5aP"
   },
   "source": [
    "### Define a convolutional autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT_BhZngWMwp"
   },
   "source": [
    "In this example, you will train a convolutional autoencoder using  [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) layers in the `encoder`, and [Conv2DTranspose](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose) layers in the `decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5KjoIlYCQko"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "\n",
    "class DenoisingAE(AE):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAE, self).__init__()\n",
    "        self.encoder = ...    \n",
    "        self.decoder = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = ...\n",
    "        decoded = ...\n",
    "        return decoded\n",
    "\n",
    "autoencoder = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the full optimization loop to optimize your denoising auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYKbiDFYCQfj"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7-VAuEy_N6M"
   },
   "source": [
    "Plot both the noisy images and the denoised images produced by the autoencoder to check that your implementation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfxr9NdBCP_x"
   },
   "outputs": [],
   "source": [
    "######################\n",
    "# YOUR CODE GOES HERE\n",
    "######################\n",
    "encoded_imgs = ...\n",
    "decoded_imgs = ...\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ...\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<a id=\"reference1\"></a>\n",
    "[1] Rezende, Danilo Jimenez, and Shakir Mohamed. \"Variational inference with normalizing flows.\" _arXiv preprint arXiv:1505.05770_ (2015). [link](http://arxiv.org/pdf/1505.05770)\n",
    "\n",
    "[2] Kingma, Diederik P., Tim Salimans, and Max Welling. \"Improving Variational Inference with Inverse Autoregressive Flow.\" _arXiv preprint arXiv:1606.04934_ (2016). [link](https://arxiv.org/abs/1606.04934)\n",
    "\n",
    "[3] Kingma, D. P., & Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. (2013). [link](https://arxiv.org/pdf/1312.6114)\n",
    "\n",
    "[4] Rezende, D. J., Mohamed, S., & Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082. (2014). [link](https://arxiv.org/pdf/1401.4082)\n",
    "\n",
    "[5] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D., & Courville, A. (2016). Pixelvae: A latent variable model for natural images. arXiv preprint arXiv:1611.05013. [link](https://arxiv.org/pdf/1611.05013)\n",
    "\n",
    "[6] Van den Oord, A., & Vinyals, O. (2017). Neural discrete representation learning. In NIPS 2017 (pp. 6306-6315). [link](http://papers.nips.cc/paper/7210-neural-discrete-representation-learning.pdf)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TensorFlow_3_Autoencoder_Dimensionality_Reduction.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
